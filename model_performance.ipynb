{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import FirthRegression as flr\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"modified_train_set.csv\")\n",
    "X_train = train.drop(\"Y_train\",1)\n",
    "Y_train = train[\"Y_train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Variables       Coeff  Wald_p        bse\n",
      "0      const   -8.104687  0.0000   0.657659\n",
      "1         X5    6.189979  0.0000   0.875002\n",
      "2         X7   27.399041  0.0000   3.903756\n",
      "3         X8   17.091045  0.0000   3.805615\n",
      "4        X12   -1.544147  0.0005   0.446138\n",
      "5        X13   -8.540442  0.0258   3.830482\n",
      "6        X14   30.573958  0.0042  10.688401\n",
      "7        X16    8.150696  0.0000   1.306940\n",
      "8        X17   14.695503  0.0000   2.817914\n",
      "9        X19    0.619672  0.0001   0.156021\n",
      "10       X24   20.681735  0.0001   5.165991\n",
      "11       X25  -13.991555  0.0000   1.523230\n",
      "12       X27  -29.586910  0.0000   6.222494\n",
      "13       X28   19.367041  0.0015   6.088874\n",
      "14       X35  -21.507714  0.0258   9.650368\n",
      "15       X36   12.481175  0.0326   5.839085\n",
      "16       X37  -11.675334  0.0006   3.403846\n",
      "17       X38  239.491342  0.0032  81.334682\n",
      "18       X42  -33.687428  0.0000   7.355563\n",
      "19       X44  -22.353485  0.0351  10.605757\n",
      "20       X45   -6.981967  0.0000   1.298074\n",
      "21       X46  -19.217139  0.0000   3.907984\n",
      "22       X48  -67.000925  0.0217  29.176489\n",
      "23       X52    7.740718  0.0000   0.941207\n",
      "24       X53   33.328748  0.0000   3.977267\n",
      "25       X54  -33.977338  0.0011  10.438770\n",
      "26       X55    8.730150  0.0000   0.892247\n"
     ]
    }
   ],
   "source": [
    "(intercept, beta, bse, fitll, summary) = flr.fit_firth(Y_train, X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storieng all the Coefficients as a list\n",
    "coeffs = [intercept] + beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Model Performance\n",
    "In order to make Predictions on the Validation set, we first create a logistic function and use a default threshold of 0.5 to make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logit_predictions(weight, df, threshold = 0.5):\n",
    "    X = df.to_numpy().T\n",
    "    m,n = X.shape\n",
    "    wt_1 = np.reshape(np.array(weight), (1,m))\n",
    "    y_pred = wt_1@X\n",
    "    y_1 = 1./(1. + np.exp(-y_pred))\n",
    "    y_hat = np.where(y_1 > threshold,1.,0.)\n",
    "    return y_hat[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now test the performance of this model on the Validation set.\n",
    "\n",
    "**NOTE:** It is to be noted that the YeoJohnson transformation has already been performed on the validation set. A column of 1s has also been added to the set. The Unnamed column has also been removed from the set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_set = pd.read_csv(\"Validation_set.csv\")\n",
    "X_val = val_set[X_train.columns.tolist()] # Selecting only the required features\n",
    "y_val = val_set[\"y_actual\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = logit_predictions(coeffs,X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Performance Metrics\n",
    "\n",
    "We make use of various metrics to determine the performance of the model on the validation set.\n",
    "\n",
    "We consider the following : \n",
    "* TP (True Positive): Actual value = 1, predicted value = 1\n",
    "* TN (True negative): Actual value = 0, predicted value = 0\n",
    "* FP (False Positive): Actual value = 0, predicted value = 1\n",
    "* FN (False Negative): Actual value = 1, predicted value = 0\n",
    "\n",
    "We use the following metrics:\n",
    "1. Accuracy: Measures the proportion of correct predictions to the total number of predictions\n",
    "$$ formula : \\frac{TP + TN}{TP + TN + FP + FN} $$\n",
    "2. Sensitivity: The ability of the model to correctly identify a positive. Also known as Recall.\n",
    "$$ formula : \\frac{TP}{TP + FN} $$\n",
    "3. Specificity: The ability of the model to correctly identify a negative.\n",
    "$$ formula : \\frac{TN}{TN + FP} $$\n",
    "4. Precision: Proportion of true positives to the total number of positives predicted\n",
    "$$ formula : \\frac{TP}{TP + FP} $$\n",
    "5. F1-Score: The harmonic mean of Prediction and Recall (Sensitivity). The F1-score is high when there is a balance between Precision and Recall.\n",
    "$$ formula : \\frac{2*Precision*Recall}{Precision + Recall} $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_results(Y_test, y_pred):\n",
    "    cm = confusion_matrix(Y_test,y_pred)\n",
    "    print('Confusion Matrix : \\n')\n",
    "    print(pd.crosstab(y_val, y_pred, rownames = ['Actual'], colnames =['Predicted'], margins = True),'\\n')\n",
    "    total=sum(sum(cm))\n",
    "    TN = cm[0,0]\n",
    "    FP = cm[1,0]\n",
    "    FN = cm[0,1]\n",
    "    TP = cm[1,1]\n",
    "    #####from confusion matrix calculate various metrics\n",
    "    accuracy=(TP + TN)/total\n",
    "    print ('Accuracy : ', accuracy)\n",
    "    sensitivity = TP/(TP + FN)\n",
    "    print('Sensitivity : ', sensitivity )\n",
    "    specificity = TN/(TN + FP)\n",
    "    print('Specificity : ', specificity)\n",
    "    precision = (TP)/(TP + FP)\n",
    "    print('Precision : ', precision)\n",
    "    f1_score = 2*precision*sensitivity/(precision + sensitivity)\n",
    "    print('F1 - Score : ', f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix : \n",
      "\n",
      "Predicted  0.0  1.0  All\n",
      "Actual                  \n",
      "0          457   16  473\n",
      "1           28  281  309\n",
      "All        485  297  782 \n",
      "\n",
      "Accuracy :  0.9437340153452686\n",
      "Sensitivity :  0.9461279461279462\n",
      "Specificity :  0.9422680412371134\n",
      "Precision :  0.9093851132686084\n",
      "F1 - Score :  0.9273927392739274\n"
     ]
    }
   ],
   "source": [
    "prediction_results(y_val, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the model has performed well on the whole. In general, there is always a trade-off between the model's ability to detect positives/negatives better. This makes it extremely important to understand the use-case before applying any algorithm. We need to be able to prioritize the business requirements, and then check if a model's precision is more important than its ability to recall, or if it's the other way round. We can also fit various other models and compare the model performances based on these metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making Predictions on the Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"test_set.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before fitting the model, we need to convert all the variables into their log-transformations using YeoJohnson transformation,  add the intercept column, and select the features required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying log-transformations\n",
    "from scipy.stats import yeojohnson\n",
    "df_1 = pd.DataFrame()\n",
    "for col in test.columns:\n",
    "    df_1[col] = yeojohnson(test[col])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding an intercept column , selecting required features and making predictions\n",
    "import statsmodels.api as sm\n",
    "X = sm.add_constant(df_1)\n",
    "y_hat = logit_predictions(coeffs, X[X_train.columns.tolist()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing the predictions \n",
    "test['Y_preds'] = y_hat\n",
    "test.to_csv(\"Predictions.csv\",index = False)\n"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
